{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Overview\n",
    "In our previous tutorials, we only brushed upon features and how to handle them. In this overview we'll take a practical approach to learning about feature engineering. The things we will focus on are:\n",
    "- Develop a baseline model for comparing performances on models with more/different features.\n",
    "- Encode categorical features so model can make better use of the information.\n",
    "- Generate new features to provide more information for the model.\n",
    "- Select specific features to reduce overfitting and increase prediction speed.\n",
    "\n",
    "In the main exercise, we'll be using the 'TalkingDataAdTracking' kaggle competition dataset. The goal of this dataset is to predict if a user will download an app after clicking through an ad. For learning purposes we'll drop 99% of negative records (negative meaning the app wasn't downloaded) to make the target more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Baseline Model\n",
    "In this overview we'll be using Kickstarter data.\n",
    "\n",
    "### Kickstarter Warmup (review)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored # colored prints\n",
    "from my_modules import data_imports as data\n",
    "\n",
    "kst_data = data.import_kickstarter_2018_data()\n",
    "kst_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Looking at this data, let's try to predict whether or not a Kickstarter project will succeed or not. To build teach our model, we can use the *state* column as our outcome. To predict this outcome, we can use features such as category, currency, funding goal, country, and when it was launched.\n",
    "\n",
    "### Preparing target column\n",
    "First, let's look at project states and convert them into something we can use as targets in a model. Remember that model's don't like to work with strings, and our outcome data is categorical."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.unique(kst_data.state)\n",
    "# kst_data.groupby('state')['ID'].count()\n",
    "kst_data.groupby('state')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "So we see that our dataset has 6 unique states, with mostly failed and successful outcomes.\n",
    "\n",
    "Since our priority in this quick review is not data cleaning, we'll just go a long with this simple cleansing:\n",
    "- Drop projects that are \"live\"\n",
    "- Counting successful as ```outcome = 1```\n",
    "- Combining all other states as ```outcome = 0```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop live projects\n",
    "kst_data = kst_data.query('state != \"live\"')\n",
    "\n",
    "# Add the 'outcome' column with \"successful == 1\", everything else 0\n",
    "kst_data = kst_data.assign(outcome=(kst_data['state'] == 'successful').astype(int))\n",
    "kst_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting Timestamps\n",
    "Now that we have our outcome all setup and ready, it's time to handle dates. Let's convert the *launched* feature into something more categorical that our model can understand. We imported both *deadline* and *launched* as python Timestamp objects, so we can use the ```.dt.``` attribute on the timestamp column to get the times. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this below syntax doesn't work on a single Timestamp object. dt must be used on a column\n",
    "#    kst_data['launched'][0].dt\n",
    "\n",
    "kst_data = kst_data.assign(\n",
    "    hour=kst_data.launched.dt.hour,\n",
    "    day=kst_data.launched.dt.day,\n",
    "    month=kst_data.launched.dt.month,\n",
    "    year=kst_data.launched.dt.year\n",
    ")\n",
    "kst_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepping categorical variables\n",
    "Now we that both our outcome AND timestamp data setup, it's time to get our other categorical variables in check! For our model, we'll be using *category*, *currency*, and *country*, which all need to be converted into integer representations. We'll use scikit-learn's ```LabelEncoder``` for this."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(kst_data.groupby('category')['ID'].nunique())\n",
    "# print(kst_data.groupby('currency')['ID'].nunique())\n",
    "# print(kst_data.groupby('country')['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded_kst = kst_data[cat_features].apply(encoder.fit_transform)\n",
    "encoded_kst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great! Now let's gather all of the columns we're using for this model into a new, clean little dataframe. Because our original dataframe and our encoded dataframe have the same index, we can ```join``` them together easily."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kst_data has our hand-encoded hour, day, month, year, and outcome while 'encoded' has the labelencoded data. They both have the same index, so join join join!\n",
    "base_data = data = kst_data[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded_kst)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating training, validation, and test splits\n",
    "Ain't our data pretty? Now that's it's ready to go, it's time to split up our data into training, validation and test splits! Since this is just a quick review, let's take a simple approach just use slices of our data. We'll use 10% of the data as validation, 10% for testing, and 80% for training.\n",
    "\n",
    "**Note:** For python beginners (like me), there are extra steps/comments below to explain the indexing in the end"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "print(\"len of data: {}\".format(len(data)))\n",
    "print(\"valid size: {}\".format(valid_size))\n",
    "# The below indexing is a little confusing, so let's analyze it\n",
    "# We need 80% of the set for training\n",
    "print(\"80% of the dataset: {}\".format(round(len(data) * 0.8)))\n",
    "# This comes out to 300689, which is a difference of...\n",
    "print( \"Full data size - 80% data size: {}\".format(round(len(data) - (len(data) * 0.8))))\n",
    "# 75172! ... hmmmmmm now why are we using 2 * valid_size below?\n",
    "print(\"valid_size doubled: {}\".format(valid_size * 2))\n",
    "# They're the same!!! valid_size * 2 === the difference from above!\n",
    "# Oh ya.... valid_fraction = 0.1, so 100% - (10% * 2) = 80% .... I see\n",
    "\n",
    "# Remember that python uses the colon as [start:end] accessor, and using negatives gives us the opposite, so [:1] is from start to the first element, and [:-1] is from the start, to the end-1\n",
    "\n",
    "# start : end - (valid_size * 2), [0 : 375862 - 75173]\n",
    "train = data[:-2 * valid_size]\n",
    "# end - (valid_size*2) : end - valid_size, [375862 - 75173 : 375862 - 37586]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "# end - valid_size : end, 375862 - 37586 : 375826]\n",
    "test = data[-valid_size:]\n",
    "\n",
    "print(\"Length of train/valid/test: {}/{}/{}\".format(len(train), len(valid), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "In general, we want to be careful that each data set has the same proportion of the target classes (keep spliced data balanced). Let's print out the fraction of successful outcomes from each dataset to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above block, we used traditional Python3 string formatting.\n",
    "# Below, we use the new 3.6 F-strings!\n",
    "# The below statement would most similary equal:\n",
    "#   print(\"Outcome fraction = {:.4f}\".format(each.outcome.mean()))\n",
    "\n",
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction = {each.outcome.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we can see, each splice has around 35% true outcomes, likely because the data was well randomized beforehand. If this weren't the case, we could have used a helpful sklearn method: ```sklearn.model_selection.StratifiedShuffleSplit```.\n",
    "\n",
    "### Training a LightGBM model\n",
    "In previous examples, we used Random Regression Trees and XGBoost. This time around, we'll be using a *LightGBM* model. This is a tree-based model that typically provides the best performance, even compared to XGBoost. This time around our model won't be very optimized (as this is just a quick review) but we'll still see improvement through our feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if lightgbm can't be found, run the following command (if using conda)\n",
    "#   conda install -c conda-forge lightgbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "# Read the docs on lightgbm for more info on the parameters\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves' : 64, 'objective':'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\n",
    "print(colored(\"Good to go!\", 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions & evaluating the model\n",
    "Now that we got the model all setup and trained, let's make some predictions on the test set with the model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "And that's it for the basic baseline! Now we can move on engineering our features further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Categorical Encodings\n",
    "Now that we have a nice lil baseline model, it's time to engineer it a little more. In a previous lesson, Intermediate Machine Learning, we learned about one-hot encoding and in this overview we used basic label coding above. Now we'll learn about a few more encodings, specifically:\n",
    "- Count Encoding\n",
    "- Target Encoding\n",
    "- Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some helper function for testing our encodings. It'll be based off of lightgbm and data prep from above\n",
    "#  - Helper functions defined in 'feature_engineering.py'\n",
    "from my_modules import feature_engineering as fe\n",
    "train, valid, _ = fe.get_kickstarter_splits(base_data)\n",
    "bst = fe.train_kickstarter_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count Encoding\n",
    "Count encoding replaces each categorical value with the number of times it appears in the dataset. For this encoding, we'll use *categorical-encodings* package, specifically ```CountEncoder```. This encoder and the others in *categorical-encodings* work like scikit-learn transformers with ```.fit``` and ```.transform``` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_encoders conda install:\n",
    "#   $ conda install -c conda-forge category_encoders\n",
    "import category_encoders  as ce\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "count_enc = ce.CountEncoder()\n",
    "# kst_data from above, after timestamp encoding, pre basic encoding\n",
    "count_encoded = count_enc.fit_transform(kst_data[cat_features])\n",
    "\n",
    "category_data = base_data.join(count_encoded.add_suffix(\"_count\"))\n",
    "\n",
    "# Training and testing\n",
    "train, valid, _ = fe.get_kickstarter_splits(category_data)\n",
    "bst = fe.train_kickstarter_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "A slight increase from 0.7467 -> 0.7486\n",
    "\n",
    "### Target Encoding\n",
    "Target encoding replaces a categorical value with the average value of the target for that value of the feature. For example, given the country value \"CA\", we would calculate the average outcome for all the rows with ```country == 'CA'```. This is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences.\n",
    "\n",
    "This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. We should only learn the target encodings from the training dataset only and apply it to the other datasets.\n",
    "\n",
    "Much like ```CountEncoder```, we'll use ```TargetEncoder``` from *category_encoders*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, _ = fe.get_kickstarter_splits(category_data)\n",
    "\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "train = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n",
    "train.head()\n",
    "bst = fe.train_kickstarter_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adding target encoding on top of count encoding has given us another increase, \n",
    "0.7486 -> 0.7491\n",
    "\n",
    "### CatBoost Encoding\n",
    "Finally we'll look at CatBoost encoding. CatBoost is similar to target encoding in that it's based on the target probability for a given value. However, with CatBoost, for each row, the target probability is calculated only from the rows before it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['category', 'currency', 'country']\n",
    "cat_boost = ce.CatBoostEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, _ = fe.get_kickstarter_splits(category_data)\n",
    "cat_boost.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "train = train.join(cat_boost.transform(train[cat_features]).add_suffix(\"_cb\"))\n",
    "valid = valid.join(cat_boost.transform(valid[cat_features]).add_suffix(\"_cb\"))\n",
    "\n",
    "bst = fe.train_kickstarter_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "With our current model, CatBoost only gave us a 0.0001 improvement over target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Feature Selection"
   ]
  }
 ]
}