{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Overview\n",
    "When finished with this notebook, we'll be ready for anything pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored # colored prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1. Creating, Reading and Writing\n",
    "Pandas has two core objects, **DataFrame** and **Series**.\n",
    "\n",
    "### DateFrame\n",
    "A DataFrame is a table. It contains an array of individual *entries*, each of which has a certain *value*. Each entry corresponds to a row (or *record*) and a *column*.\n",
    "\n",
    "For example, consider the following simple DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's another example showing strings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are using the ```pd.DataFrame()``` constructor to generate these DataFrame objects. The syntax for declaring a new one is a dictionary whose keys are the column names (*Bob* and *Sue* in this example), and whose values are a list of entries. This is the standard way of constructing a new DataFrame, and the one you are most likely to encounter.\n",
    "\n",
    "\n",
    "The dictionary-list constructor assigns values to the *column labels*, but just uses an ascending count from 0 (0, 1, 2, 3, ...) for the *row labels*. Sometimes this is OK, but oftentimes we will want to assign these labels ourselves.\n",
    "\n",
    "The list of row labels used in a DataFrame is known as an **Index**. We can assign values to it by using an ```index``` parameter in our constructor:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'],\n",
    "              'Sue': ['Pretty good.', 'Bland.']},\n",
    "              index=['Product A', 'Product B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Series\n",
    "A Series, by contrast, is a sequence of data values. If a DataFrame is a table, a Series is a list. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Series is, in essence, a single column of a DataFrame. So you can assign column values to the Series the same way as before, using an ```index``` parameter. However, a Series does not have a column name, it only has one overall ```name```:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Series and the DataFrame are intimately related. It's helpful to think of a DataFrame as actually being just a bunch of Series \"glued together\". We'll see more of this below.\n",
    "\n",
    "### Reading data files\n",
    "Being able to create a DataFrame or Series by hand is handy. But, most of the time, we won't actually be creating our own data by hand. Instead, we'll be working with data that already exists.\n",
    "\n",
    "Data can be stored in any of a number of different forms and formats. By far the most basic of these is the humble CSV file. When you open a CSV file you get something that looks like this:\n",
    "```\n",
    "Product A, Product B, Product C\n",
    "30,21,9\n",
    "35,34,1\n",
    "41,11,11\n",
    "```\n",
    "Let's now set aside our toy datasets and read in a real dataset into a DataFrame. We'll use ```pd.read_csv()``` to do this."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "ign_scores = pd.read_csv(\"./datasets/data-vis/ign_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use the ```shape``` attribute to check how large a DataFrame is, and the ```head()``` function to peek the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ign_scores.shape)\n",
    "ign_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ```pd.read_csv()``` function is well-endowed, with over 30 optional parameters you can specify, like being able to specify a specific index column using ```index_col.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Indexing, Selecting, & Assigning\n",
    "Let's go over an important part of any data work, accessing our data.\n",
    "\n",
    "### Naive accessors\n",
    "Native Python objects provide good way of indexing data, which Pandas carries over to it's objects. Consider the data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import data\n",
    "import my_modules.data_imports as data\n",
    "wine_data = data.import_wine_data()\n",
    "\n",
    "# Peek the data\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Python, we can access the property of an object by accessing it as an attribute. A *book* object, for example, might have a *title* property, which we can access by calling ```book.title```. Columns in a pandas DataFrame work in much the same way.\n",
    "\n",
    "Hence to access the country property of reviews we can use:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "If we have a Python dictionary, we can access its values using the indexing ```([])``` operator. We can do the same with columns in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data['country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indexing in pandas\n",
    "The indexing operator and attribute selection are nice because they work just like they do in the rest of the Python ecosystem, making them easy to pick up and use. However, pandas has its own accessor operators, ```loc``` and ```iloc```. For more advanced operations, these are the ones we should use.\n",
    "\n",
    "#### Index-based selection\n",
    "Pandas indexing works in one of two paradigms. The first is **index-based selection**: selecting data based on its numerical position in the data. iloc follows this paradigm.\n",
    "\n",
    "To select the first row of data in a DataFrame, we may use the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both ```loc``` and ```iloc``` are row-first, column-second. This is the opposite of what we do in native Python (and all other languages), which is column-first, row-second.\n",
    "\n",
    "This means that it's marginally easier to retrieve rows, and marginally harder to get retrieve columns. \n",
    "##### iloc\n",
    "To get a column with iloc, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "On its own, the ```:``` operator, which also comes from native Python, means \"everything\". When combined with other selectors, however, it can be used to indicate a range of values. For example, to select the *country* column from just the first, second, and third row, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.iloc[:3, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, it's worth knowing that negative numbers can be used in selection. This will start counting forwards from the end of the values. So for example here are the last five elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.iloc[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Label-based selection\n",
    "The second paradigm for attribute selection is the one followed by the loc operator: **label-based selection**. In this paradigm, it's the data index value, not its position, which matters.\n",
    "\n",
    "##### loc\n",
    "For example, to get the first entry in reviews, we would now do the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the conutry of row 0\n",
    "wine_data.loc[0, 'country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "```iloc``` is conceptually simpler than ```loc``` because it ignores the dataset's indices. When we use ```iloc``` we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. ```loc```, by contrast, uses the information in the indices to do its work. Since your dataset usually has meaningful indices, it's usually easier to do things using loc instead. For example, here's one operation that's much easier using loc:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all data in the following columns\n",
    "wine_data.loc[:, ['taster_name', 'taster_twitter_handle', 'points']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Difference between ```loc``` and ```iloc```\n",
    "There's one main difference between ```loc``` and ```iloc``` and that's the way they handle their indexing schemas.\n",
    "\n",
    "```iloc``` uses the Python ```stdlib``` indexing scheme, where the first element of the range is included and the last one excluded. So ```0:10``` will select entries ```0,...,9```. ```loc```, meanwhile, indexes inclusively. So ```0:10``` will select entries ```0,...,10```.\n",
    "\n",
    "### Manipulating the index\n",
    "Manipulating the index\n",
    "Label-based selection derives its power from the labels in the index. Critically, the index we use is not immutable. We can manipulate the index in any way we see fit (or if it wasn't set during ```read_csv```) .\n",
    "\n",
    "The set_index() method can be used to do the job. Here is what happens when we set_index to the title field:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.set_index('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conditional selection\n",
    "We can use conditional statements inside of ```loc``` for more interesting ways of selecting data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.loc[wine_data.country == 'Italy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The above statement pulled ~20,000 rows, while originally there were ~130,000. About 15% of wine comes from Italy!\n",
    "\n",
    "Now let's find the highly reviewed wines in Italy. Wine is reviewd on an 80-100 point scale, so let's find wines that got atleast a 90."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.loc[(wine_data.country == 'Italy') & (wine_data.points >= 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pandas also comes with a few useful built-in conditional selectors, two of which are:\n",
    "\n",
    "#### ```isin```\n",
    "```isin``` lets you select data whose value \"is in\" a list of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.loc[wine_data.country.isin(['Italy', 'France'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ```isnull``` & ```notnull``` \n",
    "```isnull``` (and it's friend ```notnull```) let us highlight values which are (or are not) NaN.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.loc[wine_data.price.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Summary Functions and Maps\n",
    "In this section, we'll work on getting out data in the right \"shape\". Let's go ahead and get some summaries of our data.\n",
    "\n",
    "### Summary Functions\n",
    "First, let's consider the ```describe()``` function, which gives us a high-level summary of the attributes of a given column. Note that the ```describe()``` method is type aware and will changes it output based on the input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.points.describe()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.taster_name.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are some more useful commands:\n",
    "- ```mean()```: returns the the mean of the specified column\n",
    "- ```unique()```: returns all unique values of the specified column\n",
    "- ```value_counts()```: returns all unique values *and* how often they occur in the dataset for the specified column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.taster_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maps\n",
    "Maps are very useful for mapping/transforming out data. Python comes a few different mapping methods, but let's look at the two most useful.\n",
    "\n",
    "#### ```map()```\n",
    "This is your good ol' basic map function that will transform values with the given lambda method for the specified column.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_mean = wine_data.points.mean()\n",
    "wine_data.points.map(lambda p: p - point_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ```apply()```\n",
    "This is the DataFrame equivalent of map, that takes the supplied method and applies it on each row."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remean_points(row):\n",
    "    row.points = row.points - point_mean\n",
    "    return row\n",
    "\n",
    "wine_data.apply(remean_points, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "If ```apply()``` is called with ```axis='index'```, then instead of passing a function to transform each row, we would need to give a function to transform each *column*.\n",
    "\n",
    "**Note:** Both ```map()``` and ```apply()``` return new, transformed Series/DataFrames, leaving the original intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Grouping and Sorting\n",
    "Mapping is wonderful for transforming data across Series and DataFrames, but many times we want to group our data. For this, we'll often use the ```groupby()``` method.\n",
    "\n",
    "### Groupwise analysis\n",
    "For our first example of ```groupby()``` let's replicate the functionality of ```value_counts()```."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.groupby('points').points.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ```groupby()``` method first created a group of reviews which alloted the same points to the given wines. Then, for each group, we grabbed the ```points()``` column and counted how many times it appeared.\n",
    "\n",
    "We can think of each group we generate as being a slice of our DataFrame containing only data with values that match. This DataFrame is accessible to us directly using the ```apply()``` method, allowing us to then transform/work with these groups.\n",
    "\n",
    "Let's find the first wine reviewed from each winery."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.groupby('winery').apply(lambda df: df.title.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can also group by more than one column, for example, here's how we would pick out the best wine by country *and* province."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another useful grouping method is ```agg()``` which lets us run a bunch of different functions on the DataFrame at once; aggregation!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.groupby(['country']).price.agg([len, min, max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-indexes\n",
    "In all of the above examples, we've been working with DataFrame or Series objects with a single-label index. ```groupby()``` is slightly different in the fact that, depending on the operation we run, it will sometimes result in what is called a multi-index.\n",
    "\n",
    "A multi-index differs from a regular index in that it has multiple levels. For example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_reviewed = wine_data.groupby(['country', 'province']).description.agg([len])\n",
    "print(countries_reviewed)\n",
    "multi_index = countries_reviewed.index\n",
    "type(multi_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi-indices have several methods for dealing with their tiered structure which are absent for single-level indices. They also require two levels of labels to retrieve a value. \n",
    "\n",
    "The use cases for a multi-index are detailed alongside instructions on using them in the MultiIndex / Advanced Selection section of the pandas documentation.\n",
    "\n",
    "However, in general the multi-index method you will use most often is the one for converting back to a regular index, the reset_index() method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_reviewed.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sorting\n",
    "\n",
    "Sorting is pretty straight forward. Our previous grouped example ended up sorting by index. We can change the sort using ```sort_values()```. Note that ```sort_values()``` automatically sorts by ascending order. We can also sort the index by using ```sort_index()```.\n",
    "And finally, we can sort by a column at a time by specifying which columns to use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countries_reviewed.sort_values(by='len'))\n",
    "print(colored('==============================================', 'blue'))\n",
    "print(countries_reviewed.sort_values(by='len', ascending=False))\n",
    "print(colored('==============================================', 'blue'))\n",
    "print(countries_reviewed.sort_index())\n",
    "print(colored('==============================================', 'blue'))\n",
    "print(countries_reviewed.sort_values(by=['country', 'len']))\n",
    "print(colored('==============================================', 'blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Data Types and Missing Values\n",
    "Let's now talk about typing with DataFrames and how to replace values.\n",
    "\n",
    "### Dtypes\n",
    "The data type for a column in a DataFrame or a Series is known as a **dtype**. By using the ```.dtype``` method, we can grab the specific type of a column, whereas we can use ```.dtypes``` to get the type of every column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_data.price.dtype)\n",
    "print(colored('====================================', 'blue'))\n",
    "print(wine_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "One thing to stay aware of is the fact that strings are labeled as *objects* instead of a traditional string.\n",
    "\n",
    "Another useful function is ```astype()```, which we can use to transform a column of one type to another wherever such a conversion makes sense."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.points.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Missing Data\n",
    "Entries missing values are given the value *NaN*, short for \"Not a Number\". For technical reasons these NaN values are always of the float64 dtype.\n",
    "\n",
    "Pandas provides some methods specific to missing data. To select NaN entries we can use ```pd.isnull()``` (or its companion ```pd.notnull()```). We can use them to select specific rows from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data[pd.isnull(wine_data.country)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Replacing missing values is a common operation. Pandas provides a really handy method for this problem: ```fillna()```. ```fillna()``` provides a few different strategies for mitigating such data. For example, we can simply replace each NaN with an \"Unknown\":"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.region_2.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another handy method used for replacing certain values, is of course,```replace()```. Say one of the reviewers changed their twitter handles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Renaming and Combining\n",
    "In this final section, let's go over working with column names, index names, and other names.\n",
    "\n",
    "### Renaming\n",
    "The first function to look at is ```rename()```, which lets us change index and/or column names. Let's take the *points* column and rename it to *score*:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.rename(columns={'points':'score'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ```rename()``` function let's us rename the index *or* column by specifiying an *index* or *column* keyword parameter. The function supports a variety of input formats, but usually a Python dictionary is the most convenient. Let's rename the index for example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.rename(index={0: 'firstEntry', 1: 'secondEntry'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "We'll probably be renaming columns very often, but renaming index values is rather rare. For that, we'll be using ```set_index()``` instead.\n",
    "\n",
    "Both the row index and the column index can have their own name attribute. The complimentary ```rename_axis()``` method may be used to change these names. For example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining\n",
    "When performing operations on a dataset, we will sometimes need to combine different DataFrames and/or Series in non-trivial ways. Pandas has three core methods for doing this. In order of increasing complexity, these are ```concat()```, ```join()```, and ```merge()```. Most of what ```merge()``` can do can also be done more simply with ```join()```, so we will omit it and focus on the first two functions here.\n",
    "\n",
    "The simplest combining method is concat(). Given a list of elements, this function will smush those elements together along an axis.\n",
    "\n",
    "This is useful when we have data in different DataFrame or Series objects but having the same fields (columns). One example: the YouTube Videos dataset, which splits the data up based on country of origin (e.g. Canada and the UK, in this example). If we want to study multiple countries simultaneously, we can use ```concat()``` to smush them together:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_youtube = data.import_youtube_ca_data()\n",
    "print(colored(\"Canada Youtube:\", 'green'))\n",
    "print(canada_youtube.head())\n",
    "\n",
    "print(colored(\"British Youtube:\", 'green'))\n",
    "british_youtube = data.import_youtube_gb_data()\n",
    "print(british_youtube.head())\n",
    "\n",
    "print(colored(\"Concat data:\", 'blue'))\n",
    "pd.concat([canada_youtube, british_youtube])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "The middlemost combiner in terms of complexity is ```join()```. ```join()``` lets you combine different DataFrame objects which have an index in common. For example, to pull down videos that happened to be trending on the same day in both Canada and the UK, we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = canada_youtube.set_index(['title', 'trending_date'])\n",
    "right = canada_youtube.set_index(['title', 'trending_date'])\n",
    "\n",
    "left.join(right, lsuffix='_CAN', rsuffix='_UK')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The *lsuffix* and *rsuffix* parameters are necessary here because the data has the same column names in both British and Canadian datasets. If this wasn't true (because, say, we'd renamed them beforehand) we wouldn't need them."
   ]
  }
 ]
}