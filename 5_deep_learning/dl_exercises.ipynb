{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Deep Learning Overview\n",
    "\n",
    "This notebook is for notes and exercise from the Kaggle Learning 2.0 Deep Learning Course.\n",
    "\n",
    "1. [Introduction to Convolutions for Computer Vision Based DL](#conv)  \n",
    "2. [Transfer Learning](#transfer)\n",
    "3.  \n",
    "4.  \n",
    "\n",
    "<a id='conv'></a>\n",
    "## Introduction to Convolutions for Computer Vision Based DL\n",
    "\n",
    "Convolutions are a way for a computer to 'determine' information about an image. These convolutions are an array of values learned by deep learning algorithms that, when applied (mulitplied) to pixel values, can determine what that area/pixel is 'supposed' to be in a way. \n",
    "\n",
    "For example, given a 2x2 array, a [ [1.5,1.5], [-1.5,-1.5] ] convolution can determine if a square of 4 pixels (shaded from white-black) makes up a horizontal line.\n",
    "\n",
    "Let's do another example. The below cell will print a raw image as well as the output from applying a basic convolution to the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from my_modules import dl_helpers as helpers\n",
    "from termcolor import cprint # colored prints\n",
    "\n",
    "horizontal_line_conv = [[ 1, 1],\n",
    "                        [-1,-1]]\n",
    "vert_line_conv = [[1, -1],\n",
    "                  [1, -1]]\n",
    "\n",
    "# load_my_image and visualize_conv are utility functions taken from the Kaggle/learntools repository\n",
    "original_image = helpers.load_my_image('dog_1')\n",
    "helpers.visualize_conv(original_image, horizontal_line_conv, 'Horizontal Line Conv')\n",
    "helpers.visualize_conv(original_image, vert_line_conv, 'Vertical Line Conv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutions and tensorflow\n",
    "The below sections introduce and show us how to setup our environment for using convolutions, keras, and tensorflow.\n",
    "\n",
    "#### How convolutions work with DL\n",
    "The video below explains the basics of how convolutions work to 'learn' image sets and predict.  \n",
    "https://youtu.be/ToBPiUlLFEY\n",
    "\n",
    "#### How to setup code for convolution DL\n",
    "The video below explains some of the basic setup for a pre-existing model and convolutions.  \n",
    "https://youtu.be/sDG5tPtsbSA\n",
    "\n",
    "### Image Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose images to work with for our example\n",
    "from os.path import join\n",
    "\n",
    "image_dir = './images/dogs/train/'\n",
    "img_dict = {'dog_1' : \"00a338a92e4e7bf543340dc849230e75.jpg\",\n",
    "            'dog_2' : \"0b345d4f2434903c374ad8b8513a289b.jpg\",\n",
    "            'dog_3' : \"0db44ddb42bf1f97de987abe2bf01839.jpg\",\n",
    "            'dog_4' : \"01f8540fb1084107a6eb3e528f82c1aa.jpg\"}\n",
    "\n",
    "img_paths = [join(image_dir, file) for file in img_dict.values()]\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Read and Prep Images for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# conda install -c conda-forge tensorflow\n",
    "from tensorflow.python.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "image_size = 224\n",
    "\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    output = preprocess_input(img_array)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Model with Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "my_model = ResNet50(weights='./pre-trained/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "test_data = read_and_prep_images(img_paths)\n",
    "preds = my_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_modules.dl_helpers import decode_predictions\n",
    "from IPython.display import Image, display\n",
    "\n",
    "most_likely_labels = decode_predictions(preds, top=3)\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    display(Image(img_path))\n",
    "    cprint(f'Most Likely Label: {i + 1}', 'green')\n",
    "    print(most_likely_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='transfer'></a>\n",
    "## Transfer Learning\n",
    "Transfer learning is a technique of taking an existing pre-trained model and cutting off the output layer so we can quickly and effiently apply the model to other, maybe more specific, problems. An example of this is taking our previously trained model and using it to classify whether an image looks 'urban' or 'rural'.\n",
    "\n",
    "In our personal example, we'll be determining whether a photo is landscape or portrait. We'll be using a dataset of dog photos which are either horizontal or vertical. The below code will be very similar to the Kaggle example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "num_classes = 2\n",
    "resnet_weights_path = './pre-trained/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# Setup a new Sequential model\n",
    "my_new_model = Sequential()\n",
    "# Add the original pre-trained data minus the top (output) layer)\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
    "# Add our own (Dense) output layer with our 2 classes and softmax activation (probabilities)\n",
    "my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Disable the first (pre-trained) layer from training\n",
    "my_new_model.layers[0].trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "my_new_model.compile(optimize='sgd',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile Step Review\n",
    "There were three arguments supplied to the compile step, let's review them:\n",
    "- **Optimizer** determines how we determine the numerical values that make up the model. It can affect the resulting model and its predictions. \n",
    "- **Loss** determines what goal we optimize when determining numerical values in the model. It can affect the resulting model and predictions.\n",
    "- **Metrics** determines only what we print out while the model is being build, but it doesn't affect the model itself.\n",
    "\n",
    "### Fitting the Model\n",
    "We have our training and validation sets in ```'./sideways_dogs/images/train``` and ```'./sideways_dogs/images/val```. These will be used when setting up ```train_generator``` and ```validation_generator```.\n",
    "\n",
    "We have 220 training images and 217 validation images. We'll use a batch size of 10 and have 22 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 220 images belonging to 2 classes.\nFound 217 images belonging to 2 classes.\n/home/ttbot/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n22/22 [==============================] - 198s 9s/step - loss: 0.6078 - accuracy: 0.6955 - val_loss: 0.3421 - val_accuracy: 0.8750\n"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# our image size in px\n",
    "image_size = 244\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocess_input)\n",
    "\n",
    "train_path = './sideways_dogs/images/train'\n",
    "val_path = './sideways_dogs/images/val'\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "                                    directory=train_path,\n",
    "                                    target_size=(image_size, image_size),\n",
    "                                    batch_size=10,\n",
    "                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "                                        directory=val_path,\n",
    "                                        target_size=(image_size, image_size),\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "fit_stats = my_new_model.fit_generator(train_generator,\n",
    "                                       steps_per_epoch=22,\n",
    "                                       validation_data=validation_generator,\n",
    "                                       validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Data Augmentation\n",
    "**TODO:** original notes were lost, redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Model from Scratch\n",
    "**TODO:** original notes were lost, redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "#\n",
    "img_rows, img_cols = 28, 28\n",
    "#\n",
    "num_classes = 10\n",
    "\n",
    "def prep_data(raw):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    #\n",
    "    y = raw[:, 0]\n",
    "    #\n",
    "    out_y = keras.utils.to_categorical(y, num_classes)\n",
    "    #\n",
    "    x = raw[:,1:]\n",
    "    num_images = raw.shape[0] #\n",
    "    #\n",
    "    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n",
    "    #\n",
    "    out_x = out_x / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "fashion_file = \"\"\n",
    "#\n",
    "fashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\n",
    "x, y = prep_data(fashion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "# \n",
    "fashion_model = Sequential()\n",
    "# 1st\n",
    "fashion_model.add(Conv2D(12,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape=(img_rows, img_cols, 1)))\n",
    "# fashion_model.add(Conv2D(20,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "fashion_model.add(Conv2D(20,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "#\n",
    "fashion_model.add(Flatten())\n",
    "#\n",
    "fashion_model.add(Dense(100, activation='relu'))\n",
    "#\n",
    "fashion_model.add(Dense(num_classes,\n",
    "                        activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fashion_model.fit(x, y, batch_size = 100, epochs=4, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**TODO** second model notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_fashion_model = Sequential()\n",
    "second_fashion_model.add(Conv2D(9,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape=(img_rows, img_cols, 1)))\n",
    "second_fashion_model.add(Conv2D(18,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Conv2D(36,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Conv2D(18,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Flatten())\n",
    "second_fashion_model.add(Dense(90, activation='relu'))\n",
    "second_fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "second_fashion_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "second_fashion_model.fit(x, y, batch_size = 100, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*TODO* final notes "
   ]
  }
 ]
}