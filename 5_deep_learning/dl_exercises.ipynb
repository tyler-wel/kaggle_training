{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Deep Learning Overview\n",
    "\n",
    "This notebook is for notes and exercise from the Kaggle Learning 2.0 Deep Learning Course.\n",
    "\n",
    "1. [Introduction to Convolutions for Computer Vision Based DL](#conv)  \n",
    "2. [Transfer Learning](#transfer)\n",
    "3.  \n",
    "4.  \n",
    "\n",
    "<a id='conv'></a>\n",
    "## Introduction to Convolutions for Computer Vision Based DL\n",
    "\n",
    "Convolutions are a way for a computer to 'determine' information about an image. These convolutions are an array of values learned by deep learning algorithms that, when applied (mulitplied) to pixel values, can determine what that area/pixel is 'supposed' to be in a way. \n",
    "\n",
    "For example, given a 2x2 array, a [ [1.5,1.5], [-1.5,-1.5] ] convolution can determine if a square of 4 pixels (shaded from white-black) makes up a horizontal line.\n",
    "\n",
    "Let's do another example. The below cell will print a raw image as well as the output from applying a basic convolution to the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from my_modules import dl_helpers as helpers\n",
    "from termcolor import cprint # colored prints\n",
    "\n",
    "horizontal_line_conv = [[ 1, 1],\n",
    "                        [-1,-1]]\n",
    "vert_line_conv = [[1, -1],\n",
    "                  [1, -1]]\n",
    "\n",
    "# load_my_image and visualize_conv are utility functions taken from the Kaggle/learntools repository\n",
    "original_image = helpers.load_my_image('dog_1')\n",
    "helpers.visualize_conv(original_image, horizontal_line_conv, 'Horizontal Line Conv')\n",
    "helpers.visualize_conv(original_image, vert_line_conv, 'Vertical Line Conv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutions and tensorflow\n",
    "The below sections introduce and show us how to setup our environment for using convolutions, keras, and tensorflow.\n",
    "\n",
    "#### How convolutions work with DL\n",
    "The video below explains the basics of how convolutions work to 'learn' image sets and predict.  \n",
    "https://youtu.be/ToBPiUlLFEY\n",
    "\n",
    "#### How to setup code for convolution DL\n",
    "The video below explains some of the basic setup for a pre-existing model and convolutions.  \n",
    "https://youtu.be/sDG5tPtsbSA\n",
    "\n",
    "### Image Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose images to work with for our example\n",
    "from os.path import join\n",
    "\n",
    "image_dir = './images/dogs/train/'\n",
    "img_dict = {'dog_1' : \"00a338a92e4e7bf543340dc849230e75.jpg\",\n",
    "            'dog_2' : \"0b345d4f2434903c374ad8b8513a289b.jpg\",\n",
    "            'dog_3' : \"0db44ddb42bf1f97de987abe2bf01839.jpg\",\n",
    "            'dog_4' : \"01f8540fb1084107a6eb3e528f82c1aa.jpg\"}\n",
    "\n",
    "img_paths = [join(image_dir, file) for file in img_dict.values()]\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Read and Prep Images for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# conda install -c conda-forge tensorflow\n",
    "from tensorflow.python.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "image_size = 224\n",
    "\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    output = preprocess_input(img_array)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Model with Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "my_model = ResNet50(weights='./pre-trained/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "test_data = read_and_prep_images(img_paths)\n",
    "preds = my_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_modules.dl_helpers import decode_predictions\n",
    "from IPython.display import Image, display\n",
    "\n",
    "most_likely_labels = decode_predictions(preds, top=3)\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    display(Image(img_path))\n",
    "    cprint(f'Most Likely Label: {i + 1}', 'green')\n",
    "    print(most_likely_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='transfer'></a>\n",
    "## Transfer Learning\n",
    "Transfer learning is a technique of taking an existing pre-trained model and cutting off the output layer so we can quickly and effiently apply the model to other, maybe more specific, problems. An example of this is taking our previously trained model and using it to classify whether an image looks 'urban' or 'rural'.\n",
    "\n",
    "In our personal example, we'll be determining whether a photo is landscape or portrait. We'll be using a dataset of dog photos which are either horizontal or vertical. The below code will be very similar to the Kaggle example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "num_classes = 2\n",
    "resnet_weights_path = './pre-trained/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# Setup a new Sequential model\n",
    "my_new_model = Sequential()\n",
    "# Add the original pre-trained data minus the top (output) layer)\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
    "# Add our own (Dense) output layer with our 2 classes and softmax activation (probabilities)\n",
    "my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Disable the first (pre-trained) layer from training\n",
    "my_new_model.layers[0].trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "my_new_model.compile(optimize='sgd',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile Step Review\n",
    "There were three arguments supplied to the compile step, let's review them:\n",
    "- **Optimizer** determines how we determine the numerical values that make up the model. It can affect the resulting model and its predictions. \n",
    "- **Loss** determines what goal we optimize when determining numerical values in the model. It can affect the resulting model and predictions.\n",
    "- **Metrics** determines only what we print out while the model is being build, but it doesn't affect the model itself.\n",
    "\n",
    "### Fitting the Model\n",
    "We have our training and validation sets in ```'./sideways_dogs/images/train``` and ```'./sideways_dogs/images/val```. These will be used when setting up ```train_generator``` and ```validation_generator```.\n",
    "\n",
    "We have 220 training images and 217 validation images. We'll use a batch size of 10 and have 22 epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# our image size in px\n",
    "image_size = 244\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocess_input)\n",
    "\n",
    "train_path = './sideways_dogs/images/train'\n",
    "val_path = './sideways_dogs/images/val'\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "                                    directory=train_path,\n",
    "                                    target_size=(image_size, image_size),\n",
    "                                    batch_size=10,\n",
    "                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "                                        directory=val_path,\n",
    "                                        target_size=(image_size, image_size),\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "fit_stats = my_new_model.fit_generator(train_generator,\n",
    "                                       steps_per_epoch=22,\n",
    "                                       validation_data=validation_generator,\n",
    "                                       validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Data Augmentation\n",
    "Data augmentation is a good technique to use when we have a limited amount of data. Augmentation is basically taking original photos and manipulating them to give us more data. For example, take an urban photo. Flipping the photo on the horizontal will give us a completely different photo that's still an urban photo! This essentially doubles our data if we flip all the images.\n",
    "\n",
    "**Note**: While working with augmentation, we still need an un-augmented generator for our validation set. Only the training set should be augmented."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport for fun\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# our image size in px\n",
    "image_size = 244\n",
    "\n",
    "data_generator_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                    horizontal_flip = True,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1)\n",
    "data_gen_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_path = './sideways_dogs/images/train'\n",
    "val_path = './sideways_dogs/images/val'\n",
    "\n",
    "aug_train_generator = data_generator_aug.flow_from_directory(\n",
    "                                    directory=train_path,\n",
    "                                    target_size=(image_size, image_size),\n",
    "                                    batch_size=12,\n",
    "                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_gen_no_aug.flow_from_directory(\n",
    "                                        directory=val_path,\n",
    "                                        target_size=(image_size, image_size),\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "fit_stats_aug = my_new_model.fit_generator(aug_train_generator,\n",
    "                                       steps_per_epoch=22,\n",
    "                                       validation_data=validation_generator,\n",
    "                                       validation_steps=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Model from Scratch\n",
    "Now let's take everything we learned above, and guidance by our friend Kaggle and build our own model. We'll use the fashion-mnist set that tries to categorize clothing based on low res images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# basically the size of our images\n",
    "img_rows, img_cols = 28, 28\n",
    "# num of categorical classes\n",
    "num_classes = 10\n",
    "\n",
    "def prep_data(raw):\n",
    "    \"\"\"\n",
    "    Prepares fashion-mnist data \n",
    "    params:\n",
    "        raw: the raw image pixel data\n",
    "    returns:\n",
    "        out_x: predictors dataframe\n",
    "        out_y: target dataframe\n",
    "    \"\"\"\n",
    "    # get the first column (the target labels)\n",
    "    y = raw[:, 0]\n",
    "    # one_hot_encode the target labels\n",
    "    out_y = keras.utils.to_categorical(y, num_classes)\n",
    "    # get the predictors (everything except first row)\n",
    "    x = raw[:,1:]\n",
    "    num_images = raw.shape[0] # get total num of images through shape of the first* row\n",
    "    # original data has each pixel seperated into different columns\n",
    "    #  reshape so each row is back to 28x28 image shape\n",
    "    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n",
    "    # normalize the pixel data to 0~1 values\n",
    "    out_x = out_x / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "fashion_file = \"./clothes/fashion-mnist_train.csv\"\n",
    "# load the training data\n",
    "fashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\n",
    "x, y = prep_data(fashion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "# get base sequential model ready\n",
    "fashion_model = Sequential()\n",
    "# add first (input) layer \n",
    "fashion_model.add(Conv2D(12,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape=(img_rows, img_cols, 1)))\n",
    "# add more convolution layers\n",
    "fashion_model.add(Conv2D(20,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "fashion_model.add(Conv2D(20,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "# Flatten into a singular dimension (aka 2d -> 1d)\n",
    "fashion_model.add(Flatten())\n",
    "# add an extra dense layer for good measures\n",
    "fashion_model.add(Dense(100, activation='relu'))\n",
    "# output/prediction layer\n",
    "fashion_model.add(Dense(num_classes,\n",
    "                        activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile dat model\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 48000 samples, validate on 12000 samples\nEpoch 1/4\n48000/48000 [==============================] - 65s 1ms/sample - loss: 0.4887 - accuracy: 0.8227 - val_loss: 0.3643 - val_accuracy: 0.8738\nEpoch 2/4\n48000/48000 [==============================] - 62s 1ms/sample - loss: 0.3169 - accuracy: 0.8854 - val_loss: 0.3172 - val_accuracy: 0.8897\nEpoch 3/4\n48000/48000 [==============================] - 58s 1ms/sample - loss: 0.2628 - accuracy: 0.9041 - val_loss: 0.2730 - val_accuracy: 0.9028\nEpoch 4/4\n48000/48000 [==============================] - 59s 1ms/sample - loss: 0.2237 - accuracy: 0.9168 - val_loss: 0.2616 - val_accuracy: 0.9082\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f123d016490>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit it \n",
    "fashion_model.fit(x, y, batch_size = 100, epochs=4, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Our model gave us a score of **90.82%**. Now let's train a second model to compare different parameters and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 48000 samples, validate on 12000 samples\nEpoch 1/3\n48000/48000 [==============================] - 62s 1ms/sample - loss: 0.5065 - accuracy: 0.8161 - val_loss: 0.3601 - val_accuracy: 0.8726\nEpoch 2/3\n48000/48000 [==============================] - 60s 1ms/sample - loss: 0.3112 - accuracy: 0.8867 - val_loss: 0.3041 - val_accuracy: 0.8966\nEpoch 3/3\n48000/48000 [==============================] - 61s 1ms/sample - loss: 0.2536 - accuracy: 0.9055 - val_loss: 0.2645 - val_accuracy: 0.9087\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f12ac55b250>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_fashion_model = Sequential()\n",
    "second_fashion_model.add(Conv2D(9,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape=(img_rows, img_cols, 1)))\n",
    "second_fashion_model.add(Conv2D(18,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Conv2D(36,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Conv2D(18,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu'))\n",
    "second_fashion_model.add(Flatten())\n",
    "second_fashion_model.add(Dense(90, activation='relu'))\n",
    "second_fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "second_fashion_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "second_fashion_model.fit(x, y, batch_size = 100, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "And our second model gave us a score of **90.87%**, just slightly better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[9.81861830e-01, 2.59648778e-06, 5.61968598e-04, ...,\n        4.79698599e-07, 4.95854183e-05, 6.04083027e-07],\n       [6.15868748e-06, 9.99985218e-01, 2.77741123e-07, ...,\n        3.87616694e-09, 3.79854583e-07, 1.00034974e-07],\n       [1.45539064e-02, 1.63495915e-05, 7.19616055e-01, ...,\n        1.82456515e-05, 3.89483394e-05, 9.14063712e-05],\n       ...,\n       [1.70235071e-05, 1.41697853e-07, 1.33747426e-05, ...,\n        1.97756861e-04, 9.99737561e-01, 3.31930733e-06],\n       [6.81694038e-03, 1.17161699e-05, 6.94141956e-04, ...,\n        5.14560525e-05, 9.84435618e-01, 1.26561907e-04],\n       [2.89761946e-02, 7.77235627e-01, 1.25577589e-02, ...,\n        1.75265614e-05, 1.65918898e-02, 8.47225529e-05]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_test_file = \"./clothes/fashion-mnist_test.csv\"\n",
    "# load the training data\n",
    "fashion_test_data = np.loadtxt(fashion_test_file, skiprows=1, delimiter=',')\n",
    "test_x, test_y = prep_data(fashion_test_data)\n",
    "second_fashion_model.predict(test_x, batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: What do we do with the numby array of predictions"
   ]
  }
 ]
}