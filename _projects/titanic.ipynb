{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondabd50bd29c9084c3dacc271a23f266570",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic Data Challenge\n",
    "\n",
    "## Introduction\n",
    "**Kaggle Description**: \n",
    "> On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "> While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "> In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "\n",
    "**Goal**: To predict whether or not a passenger will survive the sinking of the Titanic based on provided information\n",
    "\n",
    "Let's begin! First, let's do some boilerplate setup."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# custom helpers\n",
    "from helpers.helper import get_splits\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# output\n",
    "from termcolor import cprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cprint('All Modules Imported!', 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv', index_col='PassengerId')\n",
    "test_data = pd.read_csv('./data/test.csv', index_col='PassengerId')\n",
    "\n",
    "cprint('Data Imported!', 'green')\n",
    "cprint('Training Data Example:', 'cyan')\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process\n",
    "1. Figure out which features we can safely drop/keep.\n",
    "2. Encode features that need encoding (label encoding, categorical encoding).\n",
    "3. Start feature engineering some new columns so we have a wider predicition set. \n",
    "4. Do feature selection to determine which features are not needed and find the best combination of features to use.\n",
    "5. Research and test what models would be best for our situation and train/test different models.\n",
    "6. Train and predict on the train/test sets.\n",
    "7. Finally, output everything to a new CSV.\n",
    "\n",
    "## Tools\n",
    "Our current model options are: LightGBM, RandomForestRegressor, ExtraTreesRegressor.\n",
    "\n",
    "I also want to use a pipeline to keep everything organized into various steps.\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "### Feature Engineering\n",
    "So the columns we have are:\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "| ----- | --- | --- |\n",
    "| survival | Survived or not | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex | |\n",
    "| age | Age in years | |\n",
    "| sibsp | Num of siblings / spouses aboard | |\n",
    "| parch | Num of parents / children aboard | |\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passengar fare | |\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of embarkation |   C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "Looking at these descriptions, we can probably disregard\n",
    "- Name\n",
    "- Ticket number\n",
    "\n",
    "Ticket number is a ... maybe, as we aren't entirely sure how ticket numbers are handed out.\n",
    "\n",
    "Let's get to engineering.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Createion Steps\n",
    "1. [x] Choose feature cols based on feature table and relevant data\n",
    "2. [x] Split into train/valid/test sets\n",
    "3. [ ] Generate features\n",
    "    1. Interactions\n",
    "    2. ...\n",
    "4. [ ] Setup pipeline\n",
    "    1. [x] Imputation to fill in N/A values\n",
    "    2. [x] Categorical encoding, CatBoost\n",
    "    4. [x] Standardize values\n",
    "    5. [ ] Feature Selection\n",
    "5. [ ] Train"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Generation / Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose our feature cols based on feature table above\n",
    "numerical_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_cols = ['Pclass', 'Sex', 'Cabin', 'Embarked']\n",
    "target_col = 'Survived'\n",
    "# Let's make some features\n",
    "from itertools import combinations\n",
    "\n",
    "interactions = pd.DataFrame(index=train_data.index)\n",
    "for comb in combinations(categorical_cols, 2):\n",
    "    new_feat = comb[0] + \"_\" + comb[1]\n",
    "    interactions[new_feat] = train_data[comb[0]].astype(str) + \"_\" + train_data[comb[1]].astype(str)\n",
    "    categorical_cols.append(new_feat)\n",
    "train_data = train_data.join(interactions)\n",
    "display(train_data)\n",
    "# 2. Split sets\n",
    "train, valid, _ = get_splits(train_data)\n",
    "X_train = train.drop([target_col], axis=1)\n",
    "y_train = train[target_col]\n",
    "X_valid = valid.drop([target_col], axis=1)\n",
    "y_valid = valid[target_col]\n",
    "display(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn import feature_selection\n",
    "from sklearn import preprocessing\n",
    "# Pipeline code originally copied from 3_intermediate_training_summary\n",
    "from helpers.helper import PipelineFS\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# conda install -c conda-forge category_encoders\n",
    "from category_encoders import CatBoostEncoder\n",
    "# conda install -c conda-forge lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_pipeline_score(X, y, params={'n_estimators':10,'num_leaves':64,'rate':0.1,'early_stopping_rounds':10}):\n",
    "    \"\"\"\n",
    "    Run LightBGM pipeline on the provided parameters. \n",
    "    Scores based on cross_validation with 5 folds.\n",
    "\n",
    "    params: Python object of params with the following keys\n",
    "        n_estimators: number of estimators to use in pipeline, Default: 10\n",
    "        num_leaves: num_leaves in lgb model, Default: 64\n",
    "        rate: learning rate, Default: 0.1\n",
    "        early_stopping_rounds: how many rounds to stop after if low variance, Default: 10\n",
    "    \"\"\"\n",
    "    # Preprocessing for numerical data (fill in NA)\n",
    "    numerical_transformer = PipelineFS(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = PipelineFS(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('catboost', CatBoostEncoder())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = lgb.LGBMClassifier(n_estimators=params['n_estimators'], num_leaves=params['num_leaves'], learning_rate=params['rate'])\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    my_pipeline = PipelineFS(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ],\n",
    "        verbose=False\n",
    "    )\n",
    "    # Preprocessing of training data, fit model \n",
    "    # my_pipeline.fit(X_train, y_train)\n",
    "    # cprint('Fit!', 'green')\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    scores = cross_val_score(my_pipeline, X, y, cv=5)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_lgb_pipeline_score(X_train, y_train)\n",
    "\n",
    "results = {}\n",
    "params={'n_estimators':10,'num_leaves':64,'rate':0.1,'early_stopping_rounds':10}\n",
    "for i in range(50, 1001, 50):\n",
    "    params['n_estimators'] = i\n",
    "    results[i] = get_lgb_pipeline_score(X_train, y_train, params=params)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes from example kernel\n",
    "So far so good, but compared to others...not great. Below are some notes after looking at some example kernels.\n",
    "- Combine the train and test data sets into a single dataframe to make feature transformation easier.\n",
    "    - Get the ids (indexes) of both the test and train sets so that they can be extracted again later."
   ]
  }
 ]
}